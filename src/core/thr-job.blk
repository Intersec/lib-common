/***************************************************************************/
/*                                                                         */
/* Copyright 2020 INTERSEC SA                                              */
/*                                                                         */
/* Licensed under the Apache License, Version 2.0 (the "License");         */
/* you may not use this file except in compliance with the License.        */
/* You may obtain a copy of the License at                                 */
/*                                                                         */
/*     http://www.apache.org/licenses/LICENSE-2.0                          */
/*                                                                         */
/* Unless required by applicable law or agreed to in writing, software     */
/* distributed under the License is distributed on an "AS IS" BASIS,       */
/* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.*/
/* See the License for the specific language governing permissions and     */
/* limitations under the License.                                          */
/*                                                                         */
/***************************************************************************/

#include <lib-common/arith.h>
#include <lib-common/datetime.h>
#include <lib-common/el.h>
#include <lib-common/unix.h>
#include <lib-common/thr.h>

#if !defined(__x86_64__) && !defined(__i386__)
#  error "this file assumes a strict memory model and is probably buggy on !x86"
#endif

#if !defined(NDEBUG) && !defined(__has_tsan)
# define __has_thr_acc
#endif

typedef struct thr_qnode_t thr_qnode_t;
struct thr_qnode_t {
    mpsc_node_t  qnode;
    thr_job_t   *job;
    thr_syn_t   *syn;
} __attribute__((aligned(CACHE_LINE_SIZE)));

struct thr_queue_t {
    thr_job_t    run;
    thr_job_t    destroy;
    mpsc_queue_t q;
    _Atomic(ssize_t) running_on;
} __attribute__((aligned(CACHE_LINE_SIZE)));

typedef _Atomic(thr_job_t *) atomic_thr_job_t;

typedef struct thr_info_t thr_info_t;
typedef _Atomic(thr_info_t *) atomic_thr_info_t;

struct thr_info_t {
    int id;

    /** top of the deque.
     * this variable is accessed through shared_read, and modified through
     * atomic_bool_cas which ensure load/store consistency. Hence no barrier
     * is needed to read it.
     */
    atomic_uint top;
    /** bottom of the deque.
     * this variable is only modified by the current thread, hence it doesn't
     * need read barriers to read it. Though other threads do need a read
     * barrier before acces, and the owner of the deque must publish new
     * values through a write barrier.
     */
    atomic_uint bot;
    atomic_bool alive;
    bool dequeue_all;

    pthread_t  thr;
    atomic_thr_info_t next;
    char       padding_0[CACHE_LINE_SIZE];

    struct deque_entry {
        atomic_thr_job_t job;
        thr_syn_t *syn;
    } q[THR_JOB_MAX];
    char       padding_1[CACHE_LINE_SIZE];

#define NCACHE_MAX    1024
    size_t       ncache_sz;
    thr_qnode_t  ncache;

#ifdef __has_thr_acc
    struct thr_acc {
        uint64_t time;
        uint64_t ec_wait_time;
        uint64_t ec_steal_time;

        unsigned jobs_local;
        unsigned jobs_queued;
        unsigned jobs_run;
        unsigned ec_gets;
        unsigned ec_waits;
        unsigned jobs_steals;
        unsigned jobs_stealed;
        unsigned jobs_failed_steals;
        unsigned jobs_failed_dequeues;
    } acc;
#endif
};

static struct {
    atomic_bool       stopping;
    thr_evc_t         ec;
    thr_evc_t         start_bar_main;
    thr_evc_t         start_bar_thr;

    atomic_thr_info_t threads;
    _Atomic(size_t)   threads_count;
    _Atomic(size_t)   target_threads_count;
    thr_evc_t         threads_count_evc;
    _Atomic(uint64_t) threads_count_gen;
    _Atomic(uint64_t) threads_gen;
    spinlock_t        threads_lock;

    el_t              before;
    el_t              wakeel;
    thr_queue_t       main_queue;
    uint64_t          reset_time;
    proctimer_t       st;
} thr_job_g;
#define _G  thr_job_g

static thr_info_t main_thr_default_g = { .id = 0 };
static __thread thr_info_t *self_g;

size_t thr_parallelism_g;
thr_queue_t *const thr_queue_main_g = &_G.main_queue;

typedef _Atomic(thr_evc_t *) atomic_thr_evc_t;
static atomic_thr_evc_t thr0_cur_ec_g;
static bool reload_at_fork_g;

#define for_each_thread(thr)  \
    for (thr_info_t *thr = atomic_load(&thr_job_g.threads); thr; \
         thr = atomic_load(&thr->next))

/* Tracing {{{ */
#ifdef __has_thr_acc

void thr_acc_reset(void)
{
    for_each_thread(thr) {
        p_clear(&thr->acc, 1);
    }
    _G.reset_time = hardclock();
    proctimer_start(&_G.st);
}

static size_t int_width(uint64_t i)
{
    int w = 1;

    for (; i > 1; i /= 10)
        w++;
    return w;
}

void thr_acc_trace(int lvl, const char *fmt, ...)
{
    uint64_t avg, wall = hardclock() - _G.reset_time;
    unsigned waste, speedup;

    struct thr_acc total = { .time = 0, };
    struct thr_acc width = { .time = 0, };
    va_list  ap;
    SB_8k(sb);

    proctimer_stop(&_G.st);

    va_start(ap, fmt);
    sb_addvf(&sb, fmt, ap);
    va_end(ap);

    width.time = int_width(wall / 1000000);
    for_each_thread(thr) {
        struct thr_acc *acc = &thr->acc;

        total.time        += acc->time;
        total.jobs_local  += acc->jobs_local;
        total.jobs_queued += acc->jobs_queued;
        total.jobs_run    += acc->jobs_run;
        total.ec_gets     += acc->ec_gets;
        total.ec_waits    += acc->ec_waits;
        total.ec_wait_time += acc->ec_wait_time;
        total.jobs_steals   += acc->jobs_steals;
        total.jobs_stealed  += acc->jobs_stealed;
        total.jobs_failed_steals += acc->jobs_failed_steals;
        total.jobs_failed_dequeues += acc->jobs_failed_dequeues;

        width.time         = MAX(width.time,        int_width(acc->time / 1000000));
        width.jobs_local   = MAX(width.jobs_local,  int_width(acc->jobs_local));
        width.jobs_queued  = MAX(width.jobs_queued, int_width(acc->jobs_queued));
        width.jobs_run     = MAX(width.jobs_run,    int_width(acc->jobs_run));
        width.ec_gets      = MAX(width.ec_gets,     int_width(acc->ec_gets));
        width.ec_waits     = MAX(width.ec_waits,    int_width(acc->ec_waits));
        width.ec_wait_time = MAX(width.ec_wait_time, int_width(acc->ec_wait_time / 1000000));
        width.jobs_steals    = MAX(width.jobs_steals,   int_width(acc->jobs_steals));
        width.jobs_stealed   = MAX(width.jobs_stealed,   int_width(acc->jobs_stealed));
        width.jobs_failed_steals = MAX(width.jobs_failed_steals,
                                       int_width(acc->jobs_failed_steals));
        width.jobs_failed_dequeues = MAX(width.jobs_failed_dequeues,
                                       int_width(acc->jobs_failed_dequeues));
    }

    e_trace(lvl, "----- %*pM", sb.len, sb.data);

    if (total.jobs_run == 0) {
        e_trace(lvl, "----- No jobs since last reset");
        return;
    }

#define TIME_FMT_ARG(t)   (int)width.time, (int)((t) / 1000000)

    sb_reset(&sb);
    for_each_thread(thr) {
        struct thr_acc *acc = &thr->acc;

        e_trace(lvl, " %2d: %*uM, %*u queued, %*u run, %*u steals (%*u jobs, %*u failed), "
                "%*u failed dequeues, %*u failed queues, %*u gets, %*u waits (%*uM)",
                thr->id,
                TIME_FMT_ARG(acc->time),
                width.jobs_queued, acc->jobs_queued,
                width.jobs_run,    acc->jobs_run,
                width.jobs_steals, acc->jobs_steals,
                width.jobs_stealed, acc->jobs_stealed,
                width.jobs_failed_steals, acc->jobs_failed_steals,
                width.jobs_failed_dequeues, acc->jobs_failed_dequeues,
                width.jobs_local, acc->jobs_local,
                width.ec_gets,     acc->ec_gets,
                width.ec_waits,    acc->ec_waits,
                TIME_FMT_ARG(acc->ec_wait_time));
    }
    e_trace(lvl, "wall %*uM, %*u queued, %*u run, %*u steals (%*u jobs, %*u failed), "
            "%*u failed dequeues, %*u failed queues, %*u gets, %*u waits (%*uM)", TIME_FMT_ARG(wall),
            width.jobs_queued, total.jobs_queued,
            width.jobs_run,    total.jobs_run,
            width.jobs_steals, total.jobs_steals,
            width.jobs_stealed, total.jobs_stealed,
            width.jobs_failed_steals, total.jobs_failed_steals,
            width.jobs_failed_dequeues, total.jobs_failed_dequeues,
            width.jobs_local, total.jobs_local,
            width.ec_gets,     total.ec_gets,
            width.ec_waits,    total.ec_waits,
            TIME_FMT_ARG(total.ec_wait_time));
    avg     = (uint64_t)(total.time / thr_parallelism_g);
    waste   = (uint64_t)(wall - avg) * 10000 / wall;
    speedup = total.time * 100 / wall;
    e_trace(lvl, "avg  %*uM (%d.%02d%% waste, %d.%02dx speedup)",
            TIME_FMT_ARG(avg), waste / 100, waste % 100,
            speedup / 100, speedup % 100);
    e_trace(lvl, "job  %*jd cycles in avg",
            (int)width.time, total.time / total.jobs_run);
    e_trace(lvl, "cost %*jd cycles of overhead per job", (int)width.time,
            (wall * thr_parallelism_g - total.time) / total.jobs_run);
    e_trace(lvl, "     %s", proctimer_report(&_G.st, NULL));
#undef TIME_FMT_ARG
}

#endif
/* }}} */
/* atomic dequeue {{{ */

static bool job_run(thr_job_t * nonnull job, thr_syn_t *syn)
{
#ifdef __has_thr_acc
    unsigned long start = hardclock();

    self_g->acc.jobs_run++;
#endif

    if ((uintptr_t)job & 3) {
        block_t blk = (block_t)((uintptr_t)job & ~(uintptr_t)3);

        blk();
        if ((uintptr_t)job & 1)
            Block_release(blk);
    } else {
        (*job->run)(job, syn);
    }

#ifdef __has_thr_acc
    self_g->acc.time += (hardclock() - start);
#endif
    if (syn)
        thr_syn__job_done(syn);
    return true;
}

void thr_syn_schedule(thr_syn_t *syn, thr_job_t *job)
{
    unsigned bot, top;
    struct deque_entry *e;

    if (syn)
        thr_syn__job_prepare(syn);

    /* Read the current limits of the queue. Since 'bot' cannot be changed by
     * another thread and 'top' can only be moved in the direction of
     * consuming threads, 'bot' is effectively the position where the job
     * should be inserted and 'bot - top' gives an upper bound to the number of
     * job in the queue (and is the actual number if no other thread try to
     * steal a job to that one concurrently to the insertion).
     */
    bot = atomic_load(&self_g->bot);
    top = atomic_load(&self_g->top);

    /* Looks like there may be too many jobs in the queue of that thread, run
     * the new one immediately.
     */
    if (unlikely((int)(bot - top) >= THR_JOB_MAX)) {
#ifdef __has_thr_acc
        self_g->acc.jobs_local++;
#endif
        job_run(job, syn);
        return;
    }

    e = &self_g->q[bot % THR_JOB_MAX];

    /* Looks like we are inserting the job in an empty queue and that the
     * current object is still used by another thread. Run it locally.
     */
    if (atomic_load_explicit(&e->job, memory_order_acquire) != NULL) {
#ifdef __has_thr_acc
        self_g->acc.jobs_local++;
#endif
        job_run(job, syn);
        return;
    }

    /* Add the job in the queue and update bottom. Since other threads can
     * only consume jobs from the queue (increment top), we can safely add the
     * new job at q[bot] and then increment bot
     */
    e->syn = syn;
    atomic_store_explicit(&e->job, job, memory_order_release);
    atomic_store(&self_g->bot, bot + 1);

#ifdef __has_thr_acc
    self_g->acc.jobs_queued++;
#endif


    thr_ec_signal(&_G.ec);
}

void thr_schedule(thr_job_t *job)
{
    thr_syn_schedule(NULL, job);
}


/** Consume the top job of the specified thread.
 *
 * @param ti    the thread info structure of the thread to update
 * @param top__ expected value of the 'top' line.
 * @return true in case of success, false if another thread already fetched
 *         the top element.
 */
static bool thr_consume_top(thr_info_t *ti, unsigned top, unsigned count)
{
    return atomic_compare_exchange_strong_explicit(&ti->top, &top,
                                                   top + count,
                                                   memory_order_relaxed,
                                                   memory_order_relaxed);
}

static bool thr_run_deque_entry(struct deque_entry *e)
{
    thr_job_t *job = atomic_load_explicit(&e->job, memory_order_acquire);
    thr_syn_t *syn = e->syn;

    atomic_store_explicit(&e->job, NULL, memory_order_release);
    return job_run(job, syn);
}

/** Run the 'bottom' job of the queue of the current thread.
 *
 * @return true if a job has been run, false if the queue is empty.
 */
static bool thr_job_dequeue(void)
{
    unsigned top, bot;

    /* Read the bottom job and update the mark to mark that job as consumed.
     * The remaining of the function will ensure we were effectively the first
     * to reclaim the ownership of that job.
     */
    bot = atomic_load_explicit(&self_g->bot, memory_order_relaxed) - 1;
    atomic_store_explicit(&self_g->bot, bot, memory_order_seq_cst);
    atomic_thread_fence(memory_order_acq_rel);

    /* Read the top line. The memory barrier ensure that the read is effectively
     * done after updating bot and thus that other thread may have seen the
     * update of bot before touching top.
     */
    top = atomic_load_explicit(&self_g->top, memory_order_relaxed);

    /* There are remaining jobs after moving the bottom line. Top has been
     * read after bot, and since bot is refetched before each individual
     * trysteal,  we're sure 'top' cannot have reached 'bot', and thus
     * we are the owner of the job we fetched, run it.
     */
    if (likely((int)(bot - top) > 0)) {
        return thr_run_deque_entry(&self_g->q[bot % THR_JOB_MAX]);
    }

    /* 'bot' and 'top' are equal, that mean that either we're consuming the
     * last job of the queue or another thread did it. To know if the job
     * is for us, check that top hasn't been moved. If it has been moved, the
     * CAS will fail and the job is not for us, if it hasn't been moved, then
     * move it to ensure nobody else will do it, then, move back 'bot' and
     * 'top' to the same point since the queue is empty.
     */
    if (likely(bot == top)) {
        if (likely(thr_consume_top(self_g, top, 1))) {
            atomic_store_explicit(&self_g->bot, top + 1, memory_order_relaxed);
            return thr_run_deque_entry(&self_g->q[bot % THR_JOB_MAX]);
        } else {
#ifdef __has_thr_acc
            self_g->acc.jobs_failed_dequeues++;
#endif
        }
    }

    /* 'top' has been moved to the previous value of bot, this means the job
     * has already been run by another thread. Since top > bot, we're
     * sure nobody else will try to read another job and that
     * top == bot + 1 == previous value of bot. Thus, we have to restore 'bot'
     * to its previous value since we didn't get ownership of the job.
     */
    atomic_store_explicit(&self_g->bot, bot + 1, memory_order_relaxed);
    return false;
}

/** Try to steal a job from the queue of another thread.
 *
 * @param ti  The thread info structure of another thread.
 * @return 1 if a job has been stolen, 0 if the thread's queue is empty,
 *         -1 if the attempt failed (a retry may be needed)
 */
static int thr_job_try_steal(thr_info_t *ti, int depth)
{
    unsigned top, bot;

    /* Read the limits of the queue of the thread and fetch the top 'job' of
     * the queue.
     */
    top = atomic_load_explicit(&ti->top, memory_order_relaxed);
    bot = atomic_load_explicit(&ti->bot, memory_order_relaxed);

    /* If the queue does not seem to be empty, then we know we own the job if
     * and only if we can CAS top. This works because a concurrent
     * dequeue/steal will always CAS top when consuming the last job of the
     * queue. The tricky part is the concurrency between dequeue and steal
     * solved by the fact we'll always move 'bot' and 'top' in dequeue when
     * emptying the queue.
     */
    if ((int)(bot - top) > 0) {
        if (likely(thr_consume_top(ti, top, 1))) {
#ifdef __has_thr_acc
            self_g->acc.jobs_stealed += 1;
            self_g->acc.jobs_steals++;

#endif
            return thr_run_deque_entry(&ti->q[top % THR_JOB_MAX]);
        } else {
#ifdef __has_thr_acc
            self_g->acc.jobs_failed_steals++;
#endif
            return -1;
        }
    }
    return 0;
}

#undef cas_top

/* FIXME: optimize for large number of threads, with a loopless fastpath */
static int thr_job_steal(void)
{
    bool empty = true;
    int i = 1;

    for (thr_info_t *thr = atomic_load(&self_g->next); thr;
         thr = atomic_load(&thr->next))
    {
        int res = thr_job_try_steal(thr, i++);

        if (res > 0) {
            return 1;
        } else
        if (res < 0) {
            empty = false;
        }
        sched_yield();
    }

    for_each_thread(thr) {
        int res;

        if (thr == self_g) {
            break;
        }

        res = thr_job_try_steal(thr, i++);

        if (res > 0) {
            return 1;
        } else
        if (res < 0) {
            empty = false;
        }
        sched_yield();
    }

    return empty ? 0 : -1;
}

/* }}} */
/* serial queues {{{ */

static ALWAYS_INLINE thr_qnode_t *thr_qnode_of(mpsc_node_t *node)
{
    return container_of(node, thr_qnode_t, qnode);
}

static thr_qnode_t *thr_qnode_create(void)
{
    if (likely(self_g)) {
        mpsc_node_t *m = atomic_load_explicit(&self_g->ncache.qnode.next,
                                              memory_order_relaxed);

        if (likely(m)) {
            thr_qnode_t *n = thr_qnode_of(m);

            atomic_store_explicit(&self_g->ncache.qnode.next,
                                  atomic_load_explicit(&n->qnode.next,
                                                       memory_order_relaxed),
                                  memory_order_relaxed);
            self_g->ncache_sz--;
            return n;
        }
    }
    return p_new_raw(thr_qnode_t, 1);
}

static void thr_qnode_destroy(mpsc_node_t *n)
{
    if (likely(self_g && self_g->ncache_sz < NCACHE_MAX)) {
        self_g->ncache_sz++;
        atomic_store_explicit(&n->next,
                              atomic_load_explicit(&self_g->ncache.qnode.next,
                                                   memory_order_relaxed),
                              memory_order_relaxed);
        atomic_store_explicit(&self_g->ncache.qnode.next, n,
                              memory_order_relaxed);
    } else {
        free(thr_qnode_of(n));
    }
}

static void thr_queue_wipe(thr_queue_t *q)
{
    assert (mpsc_queue_looks_empty(&q->q));
}
GENERIC_DELETE(thr_queue_t, thr_queue);

static void thr_queue_run_node(mpsc_node_t *m, data_t data)
{
    thr_qnode_t *n = thr_qnode_of(m);
    thr_job_t *job = n->job;
    thr_syn_t *syn = n->syn;

    thr_qnode_destroy(m);
    job_run(job, syn);
}

#define THR_QUEUE_NOT_RUNNING  -2

static void thr_queue_drain(thr_queue_t *q)
{
    bool wipe = false;
    mpsc_it_t it;
    ssize_t id;

    if (unlikely(mpsc_queue_looks_empty(&q->q))) {
        /* The queue should not be marked empty *and* scheduled. the only
         * queue that should go through this slowpath is the main queue
         * since it's not really scheduled like the other ones
         */
        assert (q == thr_queue_main_g);
        return;
    }

    id = thr_id();
    atomic_store(&q->running_on, id);
    mpsc_queue_drain_start(&it, &q->q);
    do {
        mpsc_node_t *m = mpsc_queue_drain_fast(&it, &thr_queue_run_node,
                                               (data_t){ .ptr = NULL });
        thr_qnode_t *n = thr_qnode_of(m);

        job_run(n->job, n->syn);
        wipe = (n->job == &q->destroy);
    } while (!mpsc_queue_drain_end(&it, &thr_qnode_destroy));
    atomic_compare_exchange_strong(&q->running_on, &id,
                                   THR_QUEUE_NOT_RUNNING);

    if (wipe) {
        thr_queue_delete(&q);
    }
}

static void thr_queue_run(thr_job_t *job, thr_syn_t *syn)
{
    thr_queue_drain(container_of(job, thr_queue_t, run));
}

static void thr_queue_finalize(thr_job_t *job, thr_syn_t *syn)
{
    /*
     * Do nothing in the finalize, thr_queue_drain will perform the deletion
     * itself. This function serves as a marker that we reached the end of the
     * queue.
     */
}

static thr_queue_t *thr_queue_init(thr_queue_t *q)
{
    p_clear(q, 1);
    mpsc_queue_init(&q->q);
    q->run.run     = &thr_queue_run;
    q->destroy.run = &thr_queue_finalize;
    atomic_init(&q->running_on, THR_QUEUE_NOT_RUNNING);
    return q;
}

static void thr_wakeup_thr0(void)
{
    if (thr_id() != 0) {
        thr_evc_t *ec = atomic_exchange(&thr0_cur_ec_g, NULL);

        if (ec) {
            thr_ec_broadcast(ec);
            atomic_store(&thr0_cur_ec_g, ec);
        } else {
            el_wake_fire(_G.wakeel);
        }
    }
}

void thr_syn_queue(thr_syn_t *syn, thr_queue_t *q, thr_job_t *job)
{
    if (likely(q)) {
        thr_qnode_t *n = thr_qnode_create();

        n->job = job;
        n->syn = syn;
        if (syn)
            thr_syn__job_prepare(syn);

        if (mpsc_queue_push(&q->q, &n->qnode)) {
            if (q == thr_queue_main_g) {
                thr_wakeup_thr0();
            } else {
                thr_schedule(&q->run);
            }
        }
    } else {
        thr_syn_schedule(syn, job);
    }
}

void thr_queue_sync(thr_queue_t *q, thr_job_t *job)
{
    thr_qnode_t *n;
    thr_syn_t syn;

    if (thr_is_on_queue(q)) {
        /* We're already on the queue, so don't pass the queue again */
        job_run(job, NULL);
        return;
    }

    thr_syn_init(&syn);
    thr_syn__job_prepare(&syn);
    n = thr_qnode_create();
    n->job = job;
    n->syn = &syn;
    /*
     * if mpsc_queue_push returns 1, then we're alone !
     */
    if (mpsc_queue_push(&q->q, &n->qnode)) {
        if (q == thr_queue_main_g && thr_id() != 0) {
            thr_wakeup_thr0();
        } else {
            thr_queue_drain(q);
            thr_syn_wipe(&syn);
            return;
        }
    }
    thr_syn_wait(&syn);
    thr_syn_wipe(&syn);
}

void thr_queue(thr_queue_t *q, thr_job_t *job)
{
    thr_syn_queue(NULL, q, job);
}

thr_queue_t *thr_queue_create(void)
{
    thr_queue_t *q = p_new(thr_queue_t, 1);

    return thr_queue_init(q);
}

void thr_queue_destroy(thr_queue_t *q, bool wait)
{
    assert (q != thr_queue_main_g);
    if (wait) {
        thr_queue_sync(q, &q->destroy);
    } else {
        thr_queue(q, &q->destroy);
    }
}

bool thr_is_on_queue(thr_queue_t *q)
{
    return atomic_load(&q->running_on) == (ssize_t)thr_id()
        || (q == thr_queue_main_g && thr_id() == 0);
}

/* }}} */
/* thread run loop {{{ */

static void thr_info_init(thr_info_t *info)
{
    uint64_t key = thr_ec_get(&thr_job_g.start_bar_thr);

    self_g = info;
    self_g->thr = pthread_self();
    self_g->dequeue_all = true;
    atomic_thread_fence(memory_order_acq_rel);
    atomic_store(&self_g->alive, true);
    thr_ec_signal(&thr_job_g.start_bar_main);
    if (self_g->id != 0) {
        thr_ec_wait(&thr_job_g.start_bar_thr, key);
    }
    atomic_thread_fence(memory_order_acq_rel);
}

static void thr_info_cleanup(void *unused)
{
    mpsc_node_t *m;

    if (self_g->dequeue_all) {
        do {
            if (self_g->id == 0) {
                thr_queue_drain(thr_queue_main_g);
            }
        } while (thr_job_dequeue());
    }
    if (self_g->id == 0) {
        thr_queue_wipe(thr_queue_main_g);
    }
    while ((m = atomic_load_explicit(&self_g->ncache.qnode.next,
                                     memory_order_relaxed)))
    {
        thr_qnode_t *n = thr_qnode_of(m);

        self_g->ncache.qnode.next = n->qnode.next;
        free(n);
    }
    atomic_thread_fence(memory_order_acq_rel);
    atomic_store(&self_g->alive, false);
}

static int thr_sleep_until(thr_evc_t *ec, int timeout, bool (^cond)(void))
{
    uint64_t key = thr_ec_get(ec);
    struct timeval start_tv;

    lp_gettv(&start_tv);

    while (!cond()) {
        thr_ec_timedwait(ec, key, timeout / 10);

        if (!cond()) {
            struct timeval end_tv;

            lp_gettv(&end_tv);
            if (timeval_diffmsec(&end_tv, &start_tv) >= timeout) {
                return -1;
            }
        }
        key = thr_ec_get(ec);
    }
    return 0;
}

static void *thr_job_main(void *arg)
{
    sigset_t set;
    bool (^cond)(void) = ^bool (void) {
        /* Exit immediately if we are active or we are stopping */
        return atomic_load(&_G.target_threads_count) > (size_t)self_g->id
            || unlikely(atomic_load(&_G.stopping));
    };

    sigemptyset(&set);
    sigaddset(&set, SIGABRT);
    sigaddset(&set, SIGILL);
    sigaddset(&set, SIGFPE);
    sigaddset(&set, SIGSEGV);
    sigaddset(&set, SIGBUS);
#if defined(__linux__)
    sigaddset(&set, SIGSTKFLT);
#endif
    pthread_sigmask(SIG_UNBLOCK, &set, NULL);

    thr_info_init(arg);
    pthread_cleanup_push(thr_info_cleanup, NULL);

    /* Ensure there is no open frame on the t_stack. */
    assert (mem_stack_pool_is_at_top(&t_pool_g));

    /* explanation of thread states :
     * thread_id <  target_thread_count : active threads
     * thread_id == target_thread_count : this thread is kept on disposal
     * thread_id >  target_thread_count : these threads were created but are
     *   not used anymore, keep them for 5 seconds just in case.
     */
    while (likely(!atomic_load(&_G.stopping))) {
        int res;
        uint64_t key = 0;

        /* quarantine and obsolete threads loop */
        if (!cond()) {

            /* Eventually reset the thread-local t_pool. */
            mem_stack_pool_try_reset(&t_pool_g);

            /* We may have been woken up by the addition of a new job in the
             * queue of a thread, so we must make sure another thread will
             * take over so there is no deadlock.
             */
            thr_ec_signal(&_G.ec);
            if (thr_sleep_until(&_G.threads_count_evc, 5000, cond) < 0) {
                /* obsolete threads will be shut down after 5s */
                if (atomic_load(&_G.target_threads_count) < (size_t)self_g->id) {
                    self_g->dequeue_all = false;
                    break;
                }
                /* quarantine thread will sleep again */
                continue;
            }
        }

        /* Eventually reset the thread-local t_pool. */
        mem_stack_pool_try_reset(&t_pool_g);

        while (likely(thr_job_dequeue())) {
            continue;
        }
        if (thr_job_steal() > 0) {
            continue;
        }

        do {
            sched_yield();
            key = thr_ec_get(&_G.ec);
#ifdef __has_thr_acc
            self_g->acc.ec_gets++;
#endif
        } while ((res = thr_job_steal()) < 0);
        if (res == 0 && !atomic_load(&_G.stopping)) {
#ifdef __has_thr_acc
            unsigned long start = hardclock();

            self_g->acc.ec_waits++;
#endif
            thr_ec_wait(&_G.ec, key);

#ifdef __has_thr_acc
            self_g->acc.ec_wait_time += (hardclock() - start);
#endif
        }
    }
    pthread_cleanup_pop(1);
    return NULL;
}

/* }}} */
/* Module init / shutdown {{{ */

static void thr_fork_threads(void)
{
    pthread_attr_t attr;
    uint64_t target_threads_gen = atomic_load(&_G.threads_count_gen);

    pthread_attr_init(&attr);
    pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED);

    while (target_threads_gen != atomic_load(&_G.threads_gen)
        && !atomic_load(&_G.stopping))
    {
        sigset_t fillset;
        sigset_t old;
        pthread_t tid;
        atomic_thr_info_t *last = &_G.threads;
        uint64_t target_count = atomic_load(&_G.target_threads_count) + 1;
        uint64_t expected_key;
        uint64_t key;

        sigfillset(&fillset);
        pthread_sigmask(SIG_SETMASK, &fillset, &old);

        expected_key = thr_ec_get(&_G.start_bar_main);

        while (atomic_load(last)) {
            thr_info_t *info = atomic_load(last);

            if ((uint64_t)info->id < target_count && !atomic_load(&info->alive)) {
                assert (info->id > 0);

                if (thr_create(&tid, &attr, thr_job_main, info)) {
                    e_fatal("unable to create new thread: %m");
                }
                expected_key++;
            }
            last = &info->next;
        }

        while (atomic_load(&_G.threads_count) < target_count) {
            thr_info_t *info = p_new(thr_info_t, 1);

            info->id = atomic_fetch_add(&_G.threads_count, 1);
            atomic_store(last, info);
            last = &info->next;

            if (info->id == 0) {
                thr_info_init(info);
            } else {
                if (thr_create(&tid, &attr, thr_job_main, info)) {
                    e_fatal("unable to create new thread: %m");
                }
            }
            expected_key++;
        }

        while ((key = thr_ec_get(&_G.start_bar_main)) != expected_key) {
            thr_ec_wait(&_G.start_bar_main, key);
        }

        thr_ec_broadcast(&_G.start_bar_thr);
        pthread_sigmask(SIG_SETMASK, &old, NULL);

        atomic_store(&_G.threads_gen, target_threads_gen);
    }

    pthread_attr_destroy(&attr);
}

void thr_enter_blocking_syscall(void)
{
    if (MODULE_IS_LOADED(thr)) {
        atomic_fetch_add(&_G.target_threads_count, 1);
        atomic_fetch_add(&_G.threads_count_gen, 1);
        thr_ec_broadcast(&_G.threads_count_evc);

        spin_lock(&_G.threads_lock);
        thr_fork_threads();
        spin_unlock(&_G.threads_lock);
    }
}

void thr_exit_blocking_syscall(void)
{
    if (MODULE_IS_LOADED(thr)) {
        size_t count = atomic_fetch_sub(&_G.target_threads_count, 1);

        assert (count > thr_parallelism_g);
    }
}

bool thr_job_reload_at_fork(bool enabled)
{
    bool prev = reload_at_fork_g;

    reload_at_fork_g = enabled;

    return prev;
}

__attr_notsan__
static void thr_wipe(void)
{
    thr_info_t *thr = atomic_load(&_G.threads);

    el_unregister(&_G.before);
    el_unregister(&_G.wakeel);

    while (thr) {
        thr_info_t *next = atomic_load(&thr->next);

        p_delete(&thr);
        thr = next;
    }
    p_clear(&_G, 1);
    thr_parallelism_g = 0;
    self_g = &main_thr_default_g;
}


static void thr_on_el(el_t ev, data_t priv)
{
    thr_queue_drain(thr_queue_main_g);
}

void thr_queue_main_drain(void)
{
    assert (self_g->id == 0);
    thr_queue_drain(thr_queue_main_g);
    thr_ec_signal(&_G.ec);
}

static int thr_initialize(void *arg)
{

    if (!thr_parallelism_g) {
        const char *env = getenv("THR_MAX_PARALLELISM");
        size_t nb_cpu = sysconf(_SC_NPROCESSORS_CONF);

        if (env && *env) {
            thr_parallelism_g = atoi(env);
            if (thr_parallelism_g < 2) {
                e_fatal("invalid THR_PARALLELISM value `%s`", env);
            }
            thr_parallelism_g = MIN(thr_parallelism_g, nb_cpu);
        } else {
            thr_parallelism_g = nb_cpu;
        }
        thr_parallelism_g = MAX(thr_parallelism_g, 2);

        if (unlikely(mem_tool_is_running(MEM_TOOL_VALGRIND))) {
            thr_parallelism_g = MIN(2, thr_parallelism_g);
        }

        atomic_init(&_G.stopping, false);
        thr_ec_init(&_G.ec);
        thr_ec_init(&_G.start_bar_thr);
        thr_ec_init(&_G.start_bar_main);
        thr_ec_init(&_G.threads_count_evc);
        atomic_init(&_G.target_threads_count, thr_parallelism_g);
        atomic_init(&_G.threads_count_gen, 1);
        atomic_init(&_G.threads_gen, 0);
        atomic_init(&_G.threads_count, 0);
        thr_queue_init(thr_queue_main_g);
        _G.threads_lock = 0;

        thr_fork_threads();
        _G.before = el_unref(el_before_register(&thr_on_el, NULL));
        _G.wakeel = el_unref(el_wake_register(&thr_on_el, NULL));
    }

    return 0;
}

static int thr_shutdown(void)
{
    if (thr_parallelism_g && self_g->id == 0) {
        atomic_store(&_G.stopping, true);
        atomic_store(&_G.target_threads_count, thr_parallelism_g);
        thr_ec_broadcast(&_G.ec);
        thr_ec_broadcast(&_G.threads_count_evc);

        for_each_thread(thr) {
            if (thr->id == 0) {
                continue;
            }

            atomic_thread_fence(memory_order_acq_rel);
            while (atomic_load(&thr->alive)) {
                thr_ec_broadcast(&_G.ec);
                sched_yield();
            }
        }

        thr_info_cleanup(NULL);

        thr_wipe();
        thr_ec_wipe(&thr_job_g.start_bar_main);
        thr_ec_wipe(&thr_job_g.start_bar_thr);
        thr_ec_wipe(&thr_job_g.ec);
    }
    return 0;
}

static void thr_job_at_fork(void)
{
    thr_wipe();
    if (reload_at_fork_g) {
        thr_initialize(NULL);
    } else {
        thr_queue_init(thr_queue_main_g);
    }
}

MODULE_BEGIN(thr)
    self_g = &main_thr_default_g;

    MODULE_DEPENDS_ON(thr_hooks);
    MODULE_DEPENDS_ON(el);
    MODULE_IMPLEMENTS_VOID(at_fork_on_child, &thr_job_at_fork);
MODULE_END()

/* }}} */
/* thr_syn {{{ */

size_t thr_id(void)
{
    if (!self_g) {
        return SIZE_MAX;
    }
    return self_g->id;
}

void thr_syn_wait_until(thr_syn_t *syn, bool (^cond)(void))
{
#ifdef __has_thr_acc
    unsigned long start;
#endif

    cond = ^bool (void) {
        int pending = atomic_load(&syn->pending);

        if (cond) {
            if (!cond()) {
                /* XXX ensure pending has been fetched before executing cond,
                 * otherwise this would be racy.
                 *
                 * BTW, if this assert fails, the condition probably does not
                 * really depend on the syn to be verified.
                 */
                assert (pending);
                return false;
            }
            return true;
        }
        return pending == 0;
    };

    thr_syn__retain(syn);

    if (self_g->id == 0) {
        while (!cond()) {
            int ret;
            uint64_t key;
            thr_evc_t *ec;

            if (!mpsc_queue_looks_empty(&thr_queue_main_g->q)) {
                thr_queue_drain(thr_queue_main_g);
                continue;
            }

            atomic_store(&thr0_cur_ec_g, &syn->ec);
            key = thr_ec_get(&syn->ec);
#ifdef __has_thr_acc
            self_g->acc.ec_gets++;
#endif
            if (cond()) {
                goto cas;
            }
            if (!mpsc_queue_looks_empty(&thr_queue_main_g->q)) {
                goto cas;
            }

#ifdef __has_thr_acc
            self_g->acc.ec_waits++;
            start = hardclock();
#endif
            /* allow quarantine thread to become active even if we wait
             * less than 100ms */
            atomic_fetch_add(&_G.target_threads_count, 1);
            thr_ec_broadcast(&_G.threads_count_evc);
            ret = thr_sleep_until(&syn->ec, 100, ^bool (void) {
                return cond() || !mpsc_queue_looks_empty(&thr_queue_main_g->q);
            });
            /* Decrease thread count. If we waited more than 100ms,
             * thr_enter_blocking_syscall() will increment it again. */
            atomic_fetch_sub(&_G.target_threads_count, 1);
            if (ret < 0) {
                thr_enter_blocking_syscall();
                thr_ec_wait(&syn->ec, key);
                thr_exit_blocking_syscall();
            }
#ifdef __has_thr_acc
            self_g->acc.ec_wait_time += (hardclock() - start);
#endif
          cas:
            ec = &syn->ec;
            while (unlikely(!atomic_compare_exchange_strong(&thr0_cur_ec_g,
                                                            &ec, NULL)))
            {
                sched_yield();
                ec = &syn->ec;
            }
        }
    } else {
        uint64_t key = thr_ec_get(&syn->ec);
        int ret;

#ifdef __has_thr_acc
        self_g->acc.ec_waits++;
        start = hardclock();
#endif
        atomic_fetch_add(&_G.target_threads_count, 1);
        thr_ec_broadcast(&_G.threads_count_evc);
        ret = thr_sleep_until(&syn->ec, 100, cond);
        atomic_fetch_sub(&_G.target_threads_count, 1);
        if (ret < 0) {
            thr_enter_blocking_syscall();
            do {
                thr_ec_timedwait(&syn->ec, key, 100);
                key = thr_ec_get(&syn->ec);
            } while (!cond());
            thr_exit_blocking_syscall();
        }
#ifdef __has_thr_acc
        self_g->acc.ec_wait_time += (hardclock() - start);
#endif
    }

    thr_syn__release(syn);
}

void thr_syn_wait(thr_syn_t *syn)
{
    thr_syn_wait_until(syn, NULL);
}

void thr_syn_notify(thr_syn_t *syn, thr_queue_t *q, thr_job_t *job)
{
    thr_schedule_b(^{
        thr_syn_wait(syn);
        thr_queue(q, job);
    });
}

#define TD_GET_PTR(td)  (((uintptr_t)(td)) & 0x0000ffffffffffffull)
#define TD_GET_SEQ(td)  (((uintptr_t)(td)) & 0xffff000000000000ull)

static thr_td_t *thr_syn_get_td(thr_td_t *td)
{
    return (thr_td_t *)TD_GET_PTR(td);
}

static thr_td_t *thr_syn_next_td_ptr(thr_td_t *cur, thr_td_t *next)
{
    return (thr_td_t *)((TD_GET_SEQ(cur) + 0x1000000000000ull) | TD_GET_PTR(next));
}

void thr_syn_wipe(thr_syn_t *syn)
{
    thr_td_t *ptr = thr_syn_get_td(atomic_load(&syn->head));

    thr_syn__release(syn);
    while (unlikely(atomic_load_explicit(&syn->refcnt, memory_order_acquire))) {
        cpu_relax();
    }
    thr_ec_wipe(&syn->ec);

    Block_release_p(&syn->new_td);

    while (ptr) {
        thr_td_t *next = atomic_load(&ptr->next);

        syn->delete_td(&ptr);
        ptr = next;
    }
    Block_release_p(&syn->delete_td);
}

void thr_syn_declare_td(thr_syn_t *syn, thr_td_t *(BLOCK_CARET new_td)(void),
                        void (BLOCK_CARET delete_td)(thr_td_t **))
{
    assert (!syn->new_td);
    assert (!syn->delete_td);

    syn->new_td = Block_copy(new_td);
    syn->delete_td = Block_copy(delete_td);
}

thr_td_t *thr_syn_acquire_td(thr_syn_t *syn)
{
    thr_td_t *ptr = atomic_load(&syn->head);

    assert (syn->new_td);

    if (thr_syn_get_td(ptr)) {
        thr_td_t *next;

        do {
            next = thr_syn_next_td_ptr(ptr, atomic_load(&thr_syn_get_td(ptr)->next));
        } while (!atomic_compare_exchange_weak(&syn->head, &ptr, next)
             &&  thr_syn_get_td(ptr));
    }

    ptr = thr_syn_get_td(ptr);
    if (ptr) {
        atomic_store(&ptr->next, NULL);
        return ptr;
    }

    return syn->new_td();
}

void thr_syn_release_td(thr_syn_t *syn, thr_td_t *td)
{
    thr_td_t *ptr = atomic_load(&syn->head);
    thr_td_t *next;

    do {
        next = thr_syn_next_td_ptr(ptr, td);
        atomic_store(&td->next, thr_syn_get_td(ptr));
    } while (!atomic_compare_exchange_weak(&syn->head, &ptr, next));
}

void thr_syn_collect_td(thr_syn_t *syn, void (^collector)(const thr_td_t *td))
{
    thr_td_t *ptr = thr_syn_get_td(atomic_load(&syn->head));

    assert (atomic_load(&syn->pending) == 0);

    while (ptr) {
        collector(ptr);
        ptr = atomic_load(&ptr->next);
    }
}

/* }}} */
/* {{{ High level helpers */

struct thr_for_each_lvl1_job_t {
    thr_job_t job;
    void (^blk)(size_t pos);

    size_t pos;
};

static void thr_for_each_lvl1(thr_job_t *job, thr_syn_t *syn)
{
    const struct thr_for_each_lvl1_job_t *lvl1;

    lvl1 = container_of(job, struct thr_for_each_lvl1_job_t, job);
    lvl1->blk(lvl1->pos);
}

struct thr_for_each_lvl0_job_t {
    thr_job_t job;
    void (^blk)(size_t pos);

    struct thr_for_each_lvl1_job_t *lvl1s;
    size_t from;
    size_t to;
};

static void thr_for_each_lvl0(thr_job_t *job, thr_syn_t *syn)
{
    const struct thr_for_each_lvl0_job_t *lvl0;

    lvl0 = container_of(job, struct thr_for_each_lvl0_job_t, job);

    for (size_t i = lvl0->from; i < lvl0->to; i++) {
        struct thr_for_each_lvl1_job_t *lvl1 = &lvl0->lvl1s[i - lvl0->from];

        lvl1->job.run = &thr_for_each_lvl1;
        lvl1->blk = lvl0->blk;
        lvl1->pos = i;

        thr_syn_schedule(syn, &lvl1->job);
    }
}

void thr_for_each(size_t count, void (^blk)(size_t pos))
{
    thr_syn_t syn;
    struct thr_for_each_lvl0_job_t *lvl0s = NULL;
    struct thr_for_each_lvl1_job_t *lvl1s;

    thr_syn_init(&syn);

    lvl1s = p_new_raw(struct thr_for_each_lvl1_job_t, count);

    if (count < thr_parallelism_g * 2) {
        struct thr_for_each_lvl0_job_t thr;

        thr.job.run = &thr_for_each_lvl0;
        thr.blk = blk;
        thr.from = 0;
        thr.to = count;
        thr.lvl1s = lvl1s;
        thr_for_each_lvl0(&thr.job, &syn);
    } else {
        size_t per_thread;

        lvl0s = p_new_raw(struct thr_for_each_lvl0_job_t, thr_parallelism_g);
        per_thread = DIV_ROUND_UP(count, thr_parallelism_g);

        for (size_t i = 0; i < thr_parallelism_g; i++) {
            struct thr_for_each_lvl0_job_t *t = &lvl0s[i];

            t->job.run = &thr_for_each_lvl0;
            t->blk = blk;
            t->from = i * per_thread;
            t->to = t->from + per_thread;
            t->lvl1s = &lvl1s[t->from];

            if (t->from >= count) {
                break;
            } else
            if (t->to > count) {
                t->to = count;
            }

            thr_syn_schedule(&syn, &t->job);
        }
    }

    thr_syn_wait(&syn);
    thr_syn_wipe(&syn);

    p_delete(&lvl1s);
    p_delete(&lvl0s);
}

/* }}} */
